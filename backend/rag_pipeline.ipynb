{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94bb5b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Journal logs data\n",
    "JOURNAL_LOGS = [\n",
    "    {\n",
    "        \"date\": \"2025-09-01\",\n",
    "        \"day\": \"Monday\",\n",
    "        \"text\": \"Started the week by fixing a bug in my Cloudflare Worker code where embeddings weren't saving correctly in D1. It took me a while to trace the issue, but I realized I wasn't handling null checks for user IDs before insert. After fixing it, I ran some test queries and felt proud to see the hybrid RAG pipeline actually retrieving logs by both date and topic. I ended the day with a bus ride that gave me time to think through how to better structure my STAR stories for Amazon interviews.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-02\",\n",
    "        \"day\": \"Tuesday\",\n",
    "        \"text\": \"Today I focused on debugging foreign key constraint errors. It was frustrating at first, but then I reminded myself this is exactly what backend work looks like in the real world‚Äîslow and precise. I wrote validation logic before inserts and finally stopped the runtime errors. On the fun side, I read up on Redis streams vs. pub/sub, and I'm tempted to try streams for logging events in my journal app.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-03\",\n",
    "        \"day\": \"Wednesday\",\n",
    "        \"text\": \"I worked on SigmaTable again, adding support for computed columns. I realized that typing and generics in Python can feel clunky, but the design pattern ideas make me excited about maintainable abstractions. Later in the evening, I walked to get groceries and thought about how weirdly satisfying it is when code finally compiles without errors. It feels like a small win each time.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-04\",\n",
    "        \"day\": \"Thursday\",\n",
    "        \"text\": \"Today was all about interviews. I practiced explaining indexing and B+ trees in simple words and ran through some system design questions about scaling databases. I caught myself overcomplicating explanations, so I rewrote answers in plain language. I also polished my resume bullets‚Äîadded metrics like 30% accuracy improvement from my ColPali experiment.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-05\",\n",
    "        \"day\": \"Friday\",\n",
    "        \"text\": \"Closed the week by experimenting with Twilio voice flows. I set up a simple gather loop that repeats every 5 seconds, which made me feel like I could extend it into something more creative, like a playful AI agent. Work felt lighthearted today, which was a nice change. In the evening, I checked out sunset spots in SF online‚Äîthinking about balancing productivity with simple pleasures.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-08\",\n",
    "        \"day\": \"Monday\",\n",
    "        \"text\": \"This morning I hit a roadblock setting up Docker for a Redis stream. The container wouldn't start properly, and I spent way too much time tinkering with configs. But once it worked, I celebrated with coffee and a notebook sketch of how to integrate streams into my journal pipeline. Sometimes solving infra problems gives me as much satisfaction as writing application logic.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-09\",\n",
    "        \"day\": \"Tuesday\",\n",
    "        \"text\": \"I spent most of the day implementing token bucket logic in TypeScript. It reminded me of my earlier leaky bucket experiments, and I finally understood the subtle differences between the two. I tested with simulated requests, and it felt rewarding to watch my logs print 'allowed' or 'denied' at exactly the right time. These little simulations remind me why I enjoy backend systems so much.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-10\",\n",
    "        \"day\": \"Wednesday\",\n",
    "        \"text\": \"Today was reflective. I thought about how all my projects‚ÄîBetterHelp work, SigmaTable, Cloudflare assignment‚Äîconnect in weird ways. They each test a different skill: debugging messy legacy code, designing abstractions, or gluing services together. It can be overwhelming, but it makes me feel prepared for whatever comes in interviews.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-11\",\n",
    "        \"day\": \"Thursday\",\n",
    "        \"text\": \"I practiced SQL queries for date filtering in my journal RAG app. I realized SQLite's date functions can be tricky but powerful, especially when parsing ISO strings. After a couple of mistakes, I got my query to return exactly the entries between two days, which was super satisfying. I'm starting to see how structured queries and semantic search can complement each other.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-12\",\n",
    "        \"day\": \"Friday\",\n",
    "        \"text\": \"The week ended with me trying to build a minimal frontend for my RAG pipeline. I added a simple chat box using Tailwind and React, wired it up to my Worker, and tested semantic queries. Seeing my logs pop up in the UI was surreal‚Äîit felt like my private journal had turned into a real AI-powered app. That motivated me to think about polishing this for my assignment submission.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-15\",\n",
    "        \"day\": \"Monday\",\n",
    "        \"text\": \"Started working again on computed columns in SigmaTable. This time, I played with decorators and memoization patterns to make code cleaner. It was one of those days where theory (design patterns) directly influenced practice. I also noted how good it feels to merge pull requests without conflicts.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-16\",\n",
    "        \"day\": \"Tuesday\",\n",
    "        \"text\": \"I watched more content on vision-language models, specifically Umar Jamil's Paligemma walkthrough. It made me think about how far I could take multimodal retrieval in my own experiments. Even if I don't have compute to run big models, I feel smarter just understanding the pieces. The thought of fine-tuning a smaller model excites me.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-17\",\n",
    "        \"day\": \"Wednesday\",\n",
    "        \"text\": \"I faced another environment issue‚ÄîNode version mismatch between my local machine and Cloudflare deployment. Spent a couple of hours untangling that mess. Frustrating, but I learned about version pinning in `wrangler.jsonc` which should save me time later. It's a reminder that environment setup is as much a skill as coding itself.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-18\",\n",
    "        \"day\": \"Thursday\",\n",
    "        \"text\": \"I tried writing a cover letter draft for Quantcast. It was tough to balance enthusiasm with clarity, but I leaned on describing measurable impacts from my projects. Putting numbers into words made me realize I'm actually building a strong story. It made me hopeful about upcoming career opportunities.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-19\",\n",
    "        \"day\": \"Friday\",\n",
    "        \"text\": \"Wrapped up the week by reflecting on how much I've learned about indexing, sharding, and caching in databases. These concepts used to feel abstract, but now I can tie them back to my own experiments with journal retrieval. I ended the day by rewarding myself with Japanese ramen‚Äîand realized miso is the soul of a good broth.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-22\",\n",
    "        \"day\": \"Monday\",\n",
    "        \"text\": \"Today I worked on improving the UX of my feedback form project. Validating inputs on blur made it much smoother to use. Small details like these remind me how frontend work is about empathy as much as it is about code. It felt like progress I could actually show someone.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-23\",\n",
    "        \"day\": \"Tuesday\",\n",
    "        \"text\": \"I spent most of the day revising Amazon leadership principle answers. Tried to frame my hackathon leadership story with clearer 'impact' sentences. It's interesting how storytelling overlaps with software design: both need structure, clarity, and flow. I ended the evening with a sense of preparedness.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-24\",\n",
    "        \"day\": \"Wednesday\",\n",
    "        \"text\": \"I integrated semantic search into my hybrid query endpoint today. Watching cosine similarity actually rank my logs felt powerful. For the first time, I felt like my journal app had 'memory' in a meaningful way. It made me think about how humans retrieve memories vs. how RAG systems do.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-25\",\n",
    "        \"day\": \"Thursday\",\n",
    "        \"text\": \"Most of my day went into debugging Twilio flows again. But this time, instead of frustration, I approached it systematically‚Äîwriting test prompts, adjusting pauses, and verifying logs. The structure made me realize how much I've grown in approaching problems step by step. Felt proud of that growth.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-26\",\n",
    "        \"day\": \"Friday\",\n",
    "        \"text\": \"Ended the 20-day stretch reflecting on sunsets. I actually visited one of the SF spots I had been researching, and it lived up to the hype. Sitting there, I thought about balance: between ambition and rest, code and creativity, career and life. It was a peaceful way to close the week.\"\n",
    "    }\n",
    "];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e985b9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "\n",
    "def _simple_tokenize(text: str) -> List[str]:\n",
    "    # lowercase + split on non-word characters; tweak if you want stemming, etc.\n",
    "    return [t for t in re.split(r\"\\W+\", text.lower()) if t]\n",
    "\n",
    "class BM25:\n",
    "    \"\"\"\n",
    "    Okapi BM25 for keyword/lexical search.\n",
    "    - k1: term-frequency saturation (1.2‚Äì2.0 common)\n",
    "    - b : document-length normalization (0=no norm, 1=full norm; 0.75 common)\n",
    "    \"\"\"\n",
    "    def __init__(self, k1: float = 1.5, b: float = 0.75, tokenizer=_simple_tokenize):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.tokenize = tokenizer\n",
    "\n",
    "        # learned state\n",
    "        self.N = 0\n",
    "        self.avgdl = 0.0\n",
    "        self.doc_lens: List[int] = []\n",
    "        self.doc_tfs: List[Counter] = []\n",
    "        self.idf: Dict[str, float] = {}\n",
    "        self.vocab_df: Counter = Counter()\n",
    "\n",
    "    def fit(self, corpus: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Build BM25 stats from a list of documents (strings).\n",
    "        \"\"\"\n",
    "        self.N = len(corpus)\n",
    "        if self.N == 0:\n",
    "            # nothing to index\n",
    "            self.avgdl = 0.0\n",
    "            self.doc_lens = []\n",
    "            self.doc_tfs = []\n",
    "            self.idf = {}\n",
    "            self.vocab_df = Counter()\n",
    "            return\n",
    "\n",
    "        self.doc_tfs = []\n",
    "        self.doc_lens = []\n",
    "        self.vocab_df = Counter()\n",
    "\n",
    "        # per-doc term frequencies and document frequencies\n",
    "        for doc in corpus:\n",
    "            tokens = self.tokenize(doc)\n",
    "            tf = Counter(tokens)\n",
    "            self.doc_tfs.append(tf)\n",
    "            self.doc_lens.append(len(tokens))\n",
    "            # add unique terms in this doc to DF\n",
    "            self.vocab_df.update(set(tokens))\n",
    "\n",
    "        self.avgdl = (sum(self.doc_lens) / self.N) if self.N > 0 else 0.0\n",
    "\n",
    "        # IDF per term (classic BM25 form). Add small epsilon guard against division by zero.\n",
    "        eps = 1e-9\n",
    "        self.idf = {\n",
    "            term: math.log((self.N - df + 0.5) / (df + 0.5 + eps))\n",
    "            for term, df in self.vocab_df.items()\n",
    "        }\n",
    "\n",
    "    def _score_doc(self, q_tokens: List[str], doc_idx: int) -> float:\n",
    "        if self.N == 0 or self.avgdl == 0:\n",
    "            return 0.0\n",
    "        tf = self.doc_tfs[doc_idx]\n",
    "        dl = self.doc_lens[doc_idx]\n",
    "        K = self.k1 * (1 - self.b + self.b * (dl / self.avgdl))\n",
    "        score = 0.0\n",
    "        for t in q_tokens:\n",
    "            tf_t = tf.get(t, 0)\n",
    "            if tf_t == 0:\n",
    "                continue\n",
    "            idf_t = self.idf.get(t, 0.0)\n",
    "            # BM25 term contribution\n",
    "            score += idf_t * ((tf_t * (self.k1 + 1)) / (tf_t + K))\n",
    "        return score\n",
    "\n",
    "    def score(self, query: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Return BM25 scores for all documents for the given query.\n",
    "        \"\"\"\n",
    "        q_tokens = self.tokenize(query)\n",
    "        if not q_tokens or self.N == 0:\n",
    "            return [0.0] * self.N\n",
    "        return [self._score_doc(q_tokens, i) for i in range(self.N)]\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5) -> Tuple[List[int], List[float]]:\n",
    "        \"\"\"\n",
    "        Return (indices, scores) for the top_k docs.\n",
    "        \"\"\"\n",
    "        scores = self.score(query)\n",
    "        if not scores:\n",
    "            return [], []\n",
    "        idx = np.argsort(scores)[::-1][:top_k]\n",
    "        return idx.tolist(), [scores[i] for i in idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "344f5500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/homebrew/lib/python3.11/site-packages (5.1.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (2.3.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (4.56.2)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (0.35.1)\n",
      "Requirement already satisfied: Pillow in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/akshayamohan/Library/Python/3.11/lib/python/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/akshayamohan/Library/Python/3.11/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4b727ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import sqlite3\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98b59c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    def __init__(self, model_name: str=\"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.journal_logs = JOURNAL_LOGS\n",
    "        self.embeddings = None\n",
    "        self._bm25 = BM25()\n",
    "        self._generate_embeddings()\n",
    "        self.rebuild_bm25() \n",
    "    \n",
    "    def _generate_embeddings(self):\n",
    "        texts = [log[\"text\"] for log in self.journal_logs]\n",
    "        self.embeddings = self.model.encode(texts, convert_to_tensor=False)\n",
    "        print(f\"Generated embeddings for {len(texts)} journal entries\")\n",
    "\n",
    "    def semantic_search(self, query: str, top_k: int=5) -> List[Dict[str, Any]]:\n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"Embeddings not generated. Call _generate_embeddings() first.\")\n",
    "        \n",
    "        query_embedding = self.model.encode([query], convert_to_tensor=False)\n",
    "        similarity = np.dot(self.embeddings, query_embedding.T).flatten()\n",
    "        top_indices = np.argsort(similarity)[::-1][:top_k]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            result = self.journal_logs[idx].copy()\n",
    "            result[\"similarity_score\"] = float(similarity[idx])\n",
    "            results.append(result)\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def rebuild_bm25(self) -> None:\n",
    "        \"\"\"Rebuild BM25 index from current journal logs\"\"\"\n",
    "        corpus = [log[\"text\"] for log in self.journal_logs]\n",
    "        self._bm25.fit(corpus)\n",
    "        # print(f\"Rebuilt BM25 index for {len(corpus)} documents\")\n",
    "\n",
    "    def bm25_search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        # Make sure BM25 is built (call rebuild_bm25() whenever logs change)\n",
    "        if self._bm25.N != len(self.journal_logs):\n",
    "            self.rebuild_bm25()\n",
    "\n",
    "        indices, scores = self._bm25.search(query, top_k=top_k)\n",
    "        results = []\n",
    "        for idx, score in zip(indices, scores):\n",
    "            item = self.journal_logs[idx].copy()\n",
    "            item[\"bm25_score\"] = float(score)\n",
    "            results.append(item)\n",
    "        return results\n",
    "\n",
    "    def hybrid_search(self, query: str, top_k: int = 5, alpha: float = 0.5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Perform hybrid search combining BM25 keyword search and semantic search\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            top_k: Number of top results to return\n",
    "            alpha: Weight for BM25 (0.0 = pure semantic, 1.0 = pure BM25, 0.5 = balanced)\n",
    "        \n",
    "        Returns:\n",
    "            List of journal entries ranked by combined BM25 and semantic scores\n",
    "        \"\"\"\n",
    "        # Get BM25 results\n",
    "        bm25_results = self.bm25_search(query, top_k=top_k * 2)  # Get more for better combination\n",
    "        bm25_scores = {i: result[\"bm25_score\"] for i, result in enumerate(bm25_results)}\n",
    "        \n",
    "        # Get semantic results\n",
    "        semantic_results = self.semantic_search(query, top_k=top_k * 2)  # Get more for better combination\n",
    "        semantic_scores = {i: result[\"similarity_score\"] for i, result in enumerate(semantic_results)}\n",
    "        \n",
    "        # Normalize scores to [0, 1] range\n",
    "        if bm25_results:\n",
    "            bm25_max = max(bm25_scores.values())\n",
    "            bm25_min = min(bm25_scores.values())\n",
    "            bm25_range = bm25_max - bm25_min if bm25_max != bm25_min else 1\n",
    "            bm25_scores = {k: (v - bm25_min) / bm25_range for k, v in bm25_scores.items()}\n",
    "        \n",
    "        if semantic_results:\n",
    "            semantic_max = max(semantic_scores.values())\n",
    "            semantic_min = min(semantic_scores.values())\n",
    "            semantic_range = semantic_max - semantic_min if semantic_max != semantic_min else 1\n",
    "            semantic_scores = {k: (v - semantic_min) / semantic_range for k, v in semantic_scores.items()}\n",
    "        \n",
    "        # Combine results and calculate hybrid scores\n",
    "        combined_results = {}\n",
    "        \n",
    "        # Add BM25 results\n",
    "        for i, result in enumerate(bm25_results):\n",
    "            doc_id = f\"{result['date']}_{result['day']}\"  # Use date+day as unique ID\n",
    "            combined_results[doc_id] = {\n",
    "                'result': result,\n",
    "                'bm25_score': bm25_scores.get(i, 0.0),\n",
    "                'semantic_score': 0.0\n",
    "            }\n",
    "        \n",
    "        # Add semantic results\n",
    "        for i, result in enumerate(semantic_results):\n",
    "            doc_id = f\"{result['date']}_{result['day']}\"  # Use date+day as unique ID\n",
    "            if doc_id in combined_results:\n",
    "                combined_results[doc_id]['semantic_score'] = semantic_scores.get(i, 0.0)\n",
    "            else:\n",
    "                combined_results[doc_id] = {\n",
    "                    'result': result,\n",
    "                    'bm25_score': 0.0,\n",
    "                    'semantic_score': semantic_scores.get(i, 0.0)\n",
    "                }\n",
    "        \n",
    "        # Calculate hybrid scores and sort\n",
    "        hybrid_results = []\n",
    "        for doc_id, data in combined_results.items():\n",
    "            hybrid_score = alpha * data['bm25_score'] + (1 - alpha) * data['semantic_score']\n",
    "            result = data['result'].copy()\n",
    "            result['hybrid_score'] = hybrid_score\n",
    "            result['bm25_score'] = data['bm25_score']\n",
    "            result['semantic_score'] = data['semantic_score']\n",
    "            hybrid_results.append(result)\n",
    "        \n",
    "        # Sort by hybrid score and return top_k\n",
    "        hybrid_results.sort(key=lambda x: x['hybrid_score'], reverse=True)\n",
    "        return hybrid_results[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2a363534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Example usage of the RAG Pipeline\"\"\"\n",
    "    # Initialize RAG pipeline\n",
    "    rag = RAGPipeline()\n",
    "    \n",
    "    # Example semantic search\n",
    "    print(\"=== Semantic Search Example ===\")\n",
    "    results = rag.semantic_search(\"What was\", top_k=3)\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. {result['date']} ({result['day']}) - Score: {result['similarity_score']:.3f}\")\n",
    "        print(f\"   {result['text'][:100]}...\")\n",
    "        print()\n",
    "\n",
    "        # === BM25 Keyword Search Example ===\n",
    "    try:\n",
    "        bm25_results = rag.bm25_search(\"frustrated debugging\", top_k=3)\n",
    "        print(\"üéØ BM25 Results:\")\n",
    "        for i, result in enumerate(bm25_results, 1):\n",
    "            print(f\"  {i}. {result['date']} ({result['day']}) - BM25 Score: {result['bm25_score']:.3f}\")\n",
    "            print(f\"     {result['text'][:80]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå BM25 Error: {e}\")\n",
    "\n",
    "    # Balanced hybrid search (50% BM25, 50% semantic)\n",
    "    results = rag.hybrid_search(\"debugging database errors\", top_k=3, alpha=0.5)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4565ca52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 20 journal entries\n",
      "=== Semantic Search Example ===\n",
      "1. 2025-09-19 (Friday) - Score: 0.240\n",
      "   Wrapped up the week by reflecting on how much I've learned about indexing, sharding, and caching in ...\n",
      "\n",
      "2. 2025-09-10 (Wednesday) - Score: 0.202\n",
      "   Today was reflective. I thought about how all my projects‚ÄîBetterHelp work, SigmaTable, Cloudflare as...\n",
      "\n",
      "3. 2025-09-12 (Friday) - Score: 0.199\n",
      "   The week ended with me trying to build a minimal frontend for my RAG pipeline. I added a simple chat...\n",
      "\n",
      "üéØ BM25 Results:\n",
      "  1. 2025-09-25 (Thursday) - BM25 Score: 1.739\n",
      "     Most of my day went into debugging Twilio flows again. But this time, instead of...\n",
      "  2. 2025-09-10 (Wednesday) - BM25 Score: 1.724\n",
      "     Today was reflective. I thought about how all my projects‚ÄîBetterHelp work, Sigma...\n",
      "  3. 2025-09-02 (Tuesday) - BM25 Score: 1.450\n",
      "     Today I focused on debugging foreign key constraint errors. It was frustrating a...\n",
      "[{'date': '2025-09-02', 'day': 'Tuesday', 'text': \"Today I focused on debugging foreign key constraint errors. It was frustrating at first, but then I reminded myself this is exactly what backend work looks like in the real world‚Äîslow and precise. I wrote validation logic before inserts and finally stopped the runtime errors. On the fun side, I read up on Redis streams vs. pub/sub, and I'm tempted to try streams for logging events in my journal app.\", 'bm25_score': 1.0, 'hybrid_score': 1.0, 'semantic_score': 1.0}, {'date': '2025-09-25', 'day': 'Thursday', 'text': \"Most of my day went into debugging Twilio flows again. But this time, instead of frustration, I approached it systematically‚Äîwriting test prompts, adjusting pauses, and verifying logs. The structure made me realize how much I've grown in approaching problems step by step. Felt proud of that growth.\", 'bm25_score': 0.42386813890762665, 'hybrid_score': 0.44919862957877466, 'semantic_score': 0.4745291202499227}, {'date': '2025-09-03', 'day': 'Wednesday', 'text': 'I worked on SigmaTable again, adding support for computed columns. I realized that typing and generics in Python can feel clunky, but the design pattern ideas make me excited about maintainable abstractions. Later in the evening, I walked to get groceries and thought about how weirdly satisfying it is when code finally compiles without errors. It feels like a small win each time.', 'bm25_score': 0.4723501849546208, 'hybrid_score': 0.324175874029489, 'semantic_score': 0.17600156310435716}]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4660b141",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
