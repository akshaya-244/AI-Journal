{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ef03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Journal logs data converted from JavaScript to Python\n",
    "JOURNAL_LOGS = [\n",
    "    {\n",
    "        \"date\": \"2025-09-01\",\n",
    "        \"day\": \"Monday\",\n",
    "        \"text\": \"Started the week by fixing a bug in my Cloudflare Worker code where embeddings weren't saving correctly in D1. It took me a while to trace the issue, but I realized I wasn't handling null checks for user IDs before insert. After fixing it, I ran some test queries and felt proud to see the hybrid RAG pipeline actually retrieving logs by both date and topic. I ended the day with a bus ride that gave me time to think through how to better structure my STAR stories for Amazon interviews.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-02\",\n",
    "        \"day\": \"Tuesday\",\n",
    "        \"text\": \"Today I focused on debugging foreign key constraint errors. It was frustrating at first, but then I reminded myself this is exactly what backend work looks like in the real world—slow and precise. I wrote validation logic before inserts and finally stopped the runtime errors. On the fun side, I read up on Redis streams vs. pub/sub, and I'm tempted to try streams for logging events in my journal app.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-03\",\n",
    "        \"day\": \"Wednesday\",\n",
    "        \"text\": \"I worked on SigmaTable again, adding support for computed columns. I realized that typing and generics in Python can feel clunky, but the design pattern ideas make me excited about maintainable abstractions. Later in the evening, I walked to get groceries and thought about how weirdly satisfying it is when code finally compiles without errors. It feels like a small win each time.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-04\",\n",
    "        \"day\": \"Thursday\",\n",
    "        \"text\": \"Today was all about interviews. I practiced explaining indexing and B+ trees in simple words and ran through some system design questions about scaling databases. I caught myself overcomplicating explanations, so I rewrote answers in plain language. I also polished my resume bullets—added metrics like 30% accuracy improvement from my ColPali experiment.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-05\",\n",
    "        \"day\": \"Friday\",\n",
    "        \"text\": \"Closed the week by experimenting with Twilio voice flows. I set up a simple gather loop that repeats every 5 seconds, which made me feel like I could extend it into something more creative, like a playful AI agent. Work felt lighthearted today, which was a nice change. In the evening, I checked out sunset spots in SF online—thinking about balancing productivity with simple pleasures.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-08\",\n",
    "        \"day\": \"Monday\",\n",
    "        \"text\": \"This morning I hit a roadblock setting up Docker for a Redis stream. The container wouldn't start properly, and I spent way too much time tinkering with configs. But once it worked, I celebrated with coffee and a notebook sketch of how to integrate streams into my journal pipeline. Sometimes solving infra problems gives me as much satisfaction as writing application logic.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-09\",\n",
    "        \"day\": \"Tuesday\",\n",
    "        \"text\": \"I spent most of the day implementing token bucket logic in TypeScript. It reminded me of my earlier leaky bucket experiments, and I finally understood the subtle differences between the two. I tested with simulated requests, and it felt rewarding to watch my logs print 'allowed' or 'denied' at exactly the right time. These little simulations remind me why I enjoy backend systems so much.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-10\",\n",
    "        \"day\": \"Wednesday\",\n",
    "        \"text\": \"Today was reflective. I thought about how all my projects—BetterHelp work, SigmaTable, Cloudflare assignment—connect in weird ways. They each test a different skill: debugging messy legacy code, designing abstractions, or gluing services together. It can be overwhelming, but it makes me feel prepared for whatever comes in interviews.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-11\",\n",
    "        \"day\": \"Thursday\",\n",
    "        \"text\": \"I practiced SQL queries for date filtering in my journal RAG app. I realized SQLite's date functions can be tricky but powerful, especially when parsing ISO strings. After a couple of mistakes, I got my query to return exactly the entries between two days, which was super satisfying. I'm starting to see how structured queries and semantic search can complement each other.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-12\",\n",
    "        \"day\": \"Friday\",\n",
    "        \"text\": \"The week ended with me trying to build a minimal frontend for my RAG pipeline. I added a simple chat box using Tailwind and React, wired it up to my Worker, and tested semantic queries. Seeing my logs pop up in the UI was surreal—it felt like my private journal had turned into a real AI-powered app. That motivated me to think about polishing this for my assignment submission.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-15\",\n",
    "        \"day\": \"Monday\",\n",
    "        \"text\": \"Started working again on computed columns in SigmaTable. This time, I played with decorators and memoization patterns to make code cleaner. It was one of those days where theory (design patterns) directly influenced practice. I also noted how good it feels to merge pull requests without conflicts.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-16\",\n",
    "        \"day\": \"Tuesday\",\n",
    "        \"text\": \"I watched more content on vision-language models, specifically Umar Jamil's Paligemma walkthrough. It made me think about how far I could take multimodal retrieval in my own experiments. Even if I don't have compute to run big models, I feel smarter just understanding the pieces. The thought of fine-tuning a smaller model excites me.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-17\",\n",
    "        \"day\": \"Wednesday\",\n",
    "        \"text\": \"I faced another environment issue—Node version mismatch between my local machine and Cloudflare deployment. Spent a couple of hours untangling that mess. Frustrating, but I learned about version pinning in `wrangler.jsonc` which should save me time later. It's a reminder that environment setup is as much a skill as coding itself.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-18\",\n",
    "        \"day\": \"Thursday\",\n",
    "        \"text\": \"I tried writing a cover letter draft for Quantcast. It was tough to balance enthusiasm with clarity, but I leaned on describing measurable impacts from my projects. Putting numbers into words made me realize I'm actually building a strong story. It made me hopeful about upcoming career opportunities.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-19\",\n",
    "        \"day\": \"Friday\",\n",
    "        \"text\": \"Wrapped up the week by reflecting on how much I've learned about indexing, sharding, and caching in databases. These concepts used to feel abstract, but now I can tie them back to my own experiments with journal retrieval. I ended the day by rewarding myself with Japanese ramen—and realized miso is the soul of a good broth.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-22\",\n",
    "        \"day\": \"Monday\",\n",
    "        \"text\": \"Today I worked on improving the UX of my feedback form project. Validating inputs on blur made it much smoother to use. Small details like these remind me how frontend work is about empathy as much as it is about code. It felt like progress I could actually show someone.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-23\",\n",
    "        \"day\": \"Tuesday\",\n",
    "        \"text\": \"I spent most of the day revising Amazon leadership principle answers. Tried to frame my hackathon leadership story with clearer 'impact' sentences. It's interesting how storytelling overlaps with software design: both need structure, clarity, and flow. I ended the evening with a sense of preparedness.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-24\",\n",
    "        \"day\": \"Wednesday\",\n",
    "        \"text\": \"I integrated semantic search into my hybrid query endpoint today. Watching cosine similarity actually rank my logs felt powerful. For the first time, I felt like my journal app had 'memory' in a meaningful way. It made me think about how humans retrieve memories vs. how RAG systems do.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-25\",\n",
    "        \"day\": \"Thursday\",\n",
    "        \"text\": \"Most of my day went into debugging Twilio flows again. But this time, instead of frustration, I approached it systematically—writing test prompts, adjusting pauses, and verifying logs. The structure made me realize how much I've grown in approaching problems step by step. Felt proud of that growth.\"\n",
    "    },\n",
    "    {\n",
    "        \"date\": \"2025-09-26\",\n",
    "        \"day\": \"Friday\",\n",
    "        \"text\": \"Ended the 20-day stretch reflecting on sunsets. I actually visited one of the SF spots I had been researching, and it lived up to the hype. Sitting there, I thought about balance: between ambition and rest, code and creativity, career and life. It was a peaceful way to close the week.\"\n",
    "    }\n",
    "];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66c3bae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from sentence_transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence_transformers)\n",
      "  Downloading torch-2.8.0-cp311-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Downloading scipy-1.16.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/homebrew/lib/python3.11/site-packages (from sentence_transformers) (0.35.1)\n",
      "Collecting Pillow (from sentence_transformers)\n",
      "  Downloading pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/akshayamohan/Library/Python/3.11/lib/python/site-packages (from sentence_transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/akshayamohan/Library/Python/3.11/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading regex-2025.9.18-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.10)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.8.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n",
      "Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.9.18-cp311-cp311-macosx_11_0_arm64.whl (286 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Downloading torch-2.8.0-cp311-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 MB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.2-cp311-cp311-macosx_12_0_arm64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading scipy-1.16.2-cp311-cp311-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m126.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: mpmath, threadpoolctl, sympy, scipy, safetensors, regex, Pillow, networkx, joblib, torch, scikit-learn, tokenizers, transformers, sentence_transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/14\u001b[0m [sentence_transformers]sformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Pillow-11.3.0 joblib-1.5.2 mpmath-1.3.0 networkx-3.5 regex-2025.9.18 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 sentence_transformers-5.1.1 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.22.1 torch-2.8.0 transformers-4.56.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d84be765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (2.3.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8231f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import Dict\n",
    "from ctypes import resize\n",
    "from typing import List, Dict, Any\n",
    "from unittest import result\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import sqlite3\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, model_name: str=\"all_MiniLM-L6-v2\") -> None:\n",
    "        self.model = model_name\n",
    "        self.journal_logs = JOURNAL_LOGS\n",
    "        self.embeddings = None\n",
    "        self._generate_embeddings()\n",
    "    \n",
    "    def _generate_embeddings(self):\n",
    "        texts = [log[\"text\"] for log in self.journal_logs]\n",
    "        self.embeddings = self.model.encode(texts)\n",
    "\n",
    "    def semantic_search(self, query: str, top_k: int=5) -> List[Dict[str, Any]]:\n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"Embeddings not generated. Call _generate_embeddings() first.\")\n",
    "        \n",
    "        query_embedding = self.model.encode(query)\n",
    "\n",
    "        similarity = np.dot(self.embeddings, query_embedding.T).flatten()\n",
    "\n",
    "        top_indices = np.argsort(similarity)[::-1][:top_k]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            result = self.journal_logs[idx]\n",
    "            results.append(result)\n",
    "\n",
    "        return results\n",
    "    def date_filter(self, start_date: str = None, end_date: str = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Filter journal entries by date range\n",
    "        \n",
    "        Args:\n",
    "            start_date: Start date in YYYY-MM-DD format\n",
    "            end_date: End date in YYYY-MM-DD format\n",
    "            \n",
    "        Returns:\n",
    "            List of journal entries within the date range\n",
    "        \"\"\"\n",
    "        filtered_logs = []\n",
    "        \n",
    "        for log in self.journal_logs:\n",
    "            log_date = datetime.strptime(log[\"date\"], \"%Y-%m-%d\").date()\n",
    "            \n",
    "            if start_date:\n",
    "                start = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "                if log_date < start:\n",
    "                    continue\n",
    "            \n",
    "            if end_date:\n",
    "                end = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
    "                if log_date > end:\n",
    "                    continue\n",
    "            \n",
    "            filtered_logs.append(log)\n",
    "        \n",
    "        return filtered_logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a5ce3b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ddc39b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Example usage of the RAG Pipeline\"\"\"\n",
    "    # Initialize RAG pipeline\n",
    "    rag = RAGPipeline()\n",
    "    \n",
    "    # Example semantic search\n",
    "    print(\"=== Semantic Search Example ===\")\n",
    "    results = rag.semantic_search(\"Show me logs where I felt happy in the past week\", top_k=3)\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. {result['date']} ({result['day']}) - Score: {result['similarity_score']:.3f}\")\n",
    "        print(f\"   {result['text'][:100]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01c2eba0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "encode() argument 'encoding' must be str, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Example usage of the RAG Pipeline\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Initialize RAG pipeline\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m rag = \u001b[43mRAGPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Example semantic search\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Semantic Search Example ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mRAGPipeline.__init__\u001b[39m\u001b[34m(self, model_name)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mself\u001b[39m.journal_logs = JOURNAL_LOGS\n\u001b[32m     15\u001b[39m \u001b[38;5;28mself\u001b[39m.embeddings = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mRAGPipeline._generate_embeddings\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     19\u001b[39m     texts = [log[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.journal_logs]\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28mself\u001b[39m.embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: encode() argument 'encoding' must be str, not list"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c4fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
